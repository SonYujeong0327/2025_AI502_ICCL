{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.049422375980725276,
  "eval_steps": 500,
  "global_step": 800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000617779699759066,
      "grad_norm": 5.508808612823486,
      "learning_rate": 0.00019820000000000002,
      "loss": 74.2344,
      "step": 10
    },
    {
      "epoch": 0.001235559399518132,
      "grad_norm": 7.674802303314209,
      "learning_rate": 0.0001962,
      "loss": 71.675,
      "step": 20
    },
    {
      "epoch": 0.0018533390992771976,
      "grad_norm": 5.874457359313965,
      "learning_rate": 0.0001942,
      "loss": 68.3469,
      "step": 30
    },
    {
      "epoch": 0.002471118799036264,
      "grad_norm": 6.892030239105225,
      "learning_rate": 0.0001922,
      "loss": 65.8719,
      "step": 40
    },
    {
      "epoch": 0.0030888984987953298,
      "grad_norm": 4.545797824859619,
      "learning_rate": 0.0001902,
      "loss": 62.4469,
      "step": 50
    },
    {
      "epoch": 0.0037066781985543953,
      "grad_norm": 4.304683685302734,
      "learning_rate": 0.0001882,
      "loss": 61.3875,
      "step": 60
    },
    {
      "epoch": 0.004324457898313461,
      "grad_norm": 4.298625946044922,
      "learning_rate": 0.00018620000000000003,
      "loss": 59.9906,
      "step": 70
    },
    {
      "epoch": 0.004942237598072528,
      "grad_norm": 4.327900409698486,
      "learning_rate": 0.0001842,
      "loss": 58.0,
      "step": 80
    },
    {
      "epoch": 0.005560017297831593,
      "grad_norm": 4.120615005493164,
      "learning_rate": 0.0001822,
      "loss": 58.1219,
      "step": 90
    },
    {
      "epoch": 0.0061777969975906595,
      "grad_norm": 4.835208415985107,
      "learning_rate": 0.00018020000000000002,
      "loss": 57.4312,
      "step": 100
    },
    {
      "epoch": 0.006795576697349725,
      "grad_norm": 5.389724254608154,
      "learning_rate": 0.00017820000000000002,
      "loss": 57.5562,
      "step": 110
    },
    {
      "epoch": 0.007413356397108791,
      "grad_norm": 5.481947422027588,
      "learning_rate": 0.0001762,
      "loss": 57.2719,
      "step": 120
    },
    {
      "epoch": 0.008031136096867857,
      "grad_norm": 5.7456889152526855,
      "learning_rate": 0.0001742,
      "loss": 56.2875,
      "step": 130
    },
    {
      "epoch": 0.008648915796626922,
      "grad_norm": 8.535005569458008,
      "learning_rate": 0.0001722,
      "loss": 57.3094,
      "step": 140
    },
    {
      "epoch": 0.009266695496385988,
      "grad_norm": 5.398996829986572,
      "learning_rate": 0.00017020000000000002,
      "loss": 57.3687,
      "step": 150
    },
    {
      "epoch": 0.009884475196145055,
      "grad_norm": 5.05042028427124,
      "learning_rate": 0.0001682,
      "loss": 57.45,
      "step": 160
    },
    {
      "epoch": 0.01050225489590412,
      "grad_norm": 6.570258617401123,
      "learning_rate": 0.0001662,
      "loss": 56.6844,
      "step": 170
    },
    {
      "epoch": 0.011120034595663186,
      "grad_norm": 4.630506992340088,
      "learning_rate": 0.0001642,
      "loss": 56.4,
      "step": 180
    },
    {
      "epoch": 0.011737814295422252,
      "grad_norm": 5.764551639556885,
      "learning_rate": 0.0001622,
      "loss": 56.8875,
      "step": 190
    },
    {
      "epoch": 0.012355593995181319,
      "grad_norm": 5.305571556091309,
      "learning_rate": 0.00016020000000000002,
      "loss": 54.5812,
      "step": 200
    },
    {
      "epoch": 0.012973373694940385,
      "grad_norm": 5.244218826293945,
      "learning_rate": 0.00015820000000000002,
      "loss": 55.7875,
      "step": 210
    },
    {
      "epoch": 0.01359115339469945,
      "grad_norm": 5.955988883972168,
      "learning_rate": 0.0001562,
      "loss": 55.45,
      "step": 220
    },
    {
      "epoch": 0.014208933094458516,
      "grad_norm": 5.293010234832764,
      "learning_rate": 0.0001542,
      "loss": 55.0156,
      "step": 230
    },
    {
      "epoch": 0.014826712794217581,
      "grad_norm": 5.40883731842041,
      "learning_rate": 0.0001522,
      "loss": 55.1906,
      "step": 240
    },
    {
      "epoch": 0.015444492493976648,
      "grad_norm": 5.55752420425415,
      "learning_rate": 0.00015020000000000002,
      "loss": 55.1594,
      "step": 250
    },
    {
      "epoch": 0.016062272193735714,
      "grad_norm": 5.494096755981445,
      "learning_rate": 0.0001482,
      "loss": 55.5344,
      "step": 260
    },
    {
      "epoch": 0.01668005189349478,
      "grad_norm": 6.211816310882568,
      "learning_rate": 0.0001462,
      "loss": 54.3563,
      "step": 270
    },
    {
      "epoch": 0.017297831593253845,
      "grad_norm": 5.687018394470215,
      "learning_rate": 0.0001442,
      "loss": 55.4813,
      "step": 280
    },
    {
      "epoch": 0.01791561129301291,
      "grad_norm": 5.388844966888428,
      "learning_rate": 0.0001422,
      "loss": 56.2062,
      "step": 290
    },
    {
      "epoch": 0.018533390992771976,
      "grad_norm": 5.811821937561035,
      "learning_rate": 0.0001402,
      "loss": 55.6531,
      "step": 300
    },
    {
      "epoch": 0.019151170692531045,
      "grad_norm": 6.863629341125488,
      "learning_rate": 0.0001382,
      "loss": 55.5312,
      "step": 310
    },
    {
      "epoch": 0.01976895039229011,
      "grad_norm": 5.454483509063721,
      "learning_rate": 0.0001362,
      "loss": 56.2969,
      "step": 320
    },
    {
      "epoch": 0.020386730092049176,
      "grad_norm": 6.352076053619385,
      "learning_rate": 0.0001342,
      "loss": 55.3,
      "step": 330
    },
    {
      "epoch": 0.02100450979180824,
      "grad_norm": 7.709288120269775,
      "learning_rate": 0.00013220000000000001,
      "loss": 54.7437,
      "step": 340
    },
    {
      "epoch": 0.021622289491567307,
      "grad_norm": 5.768813610076904,
      "learning_rate": 0.00013020000000000002,
      "loss": 54.5812,
      "step": 350
    },
    {
      "epoch": 0.022240069191326373,
      "grad_norm": 4.954456329345703,
      "learning_rate": 0.0001282,
      "loss": 54.5125,
      "step": 360
    },
    {
      "epoch": 0.022857848891085438,
      "grad_norm": 5.2157368659973145,
      "learning_rate": 0.0001262,
      "loss": 53.9375,
      "step": 370
    },
    {
      "epoch": 0.023475628590844504,
      "grad_norm": 4.81162166595459,
      "learning_rate": 0.0001242,
      "loss": 55.2531,
      "step": 380
    },
    {
      "epoch": 0.02409340829060357,
      "grad_norm": 6.751106262207031,
      "learning_rate": 0.00012220000000000002,
      "loss": 53.9156,
      "step": 390
    },
    {
      "epoch": 0.024711187990362638,
      "grad_norm": 5.8293352127075195,
      "learning_rate": 0.00012020000000000001,
      "loss": 55.0938,
      "step": 400
    },
    {
      "epoch": 0.025328967690121704,
      "grad_norm": 5.829359531402588,
      "learning_rate": 0.0001182,
      "loss": 53.6187,
      "step": 410
    },
    {
      "epoch": 0.02594674738988077,
      "grad_norm": 5.995086669921875,
      "learning_rate": 0.00011619999999999999,
      "loss": 54.7531,
      "step": 420
    },
    {
      "epoch": 0.026564527089639835,
      "grad_norm": 5.541776180267334,
      "learning_rate": 0.0001142,
      "loss": 54.3625,
      "step": 430
    },
    {
      "epoch": 0.0271823067893989,
      "grad_norm": 5.626079082489014,
      "learning_rate": 0.00011220000000000002,
      "loss": 55.3375,
      "step": 440
    },
    {
      "epoch": 0.027800086489157966,
      "grad_norm": 6.383639812469482,
      "learning_rate": 0.00011020000000000001,
      "loss": 54.4844,
      "step": 450
    },
    {
      "epoch": 0.02841786618891703,
      "grad_norm": 5.652544021606445,
      "learning_rate": 0.00010820000000000001,
      "loss": 54.1969,
      "step": 460
    },
    {
      "epoch": 0.029035645888676097,
      "grad_norm": 5.778069019317627,
      "learning_rate": 0.0001062,
      "loss": 54.5625,
      "step": 470
    },
    {
      "epoch": 0.029653425588435162,
      "grad_norm": 5.472870826721191,
      "learning_rate": 0.00010420000000000001,
      "loss": 55.7812,
      "step": 480
    },
    {
      "epoch": 0.03027120528819423,
      "grad_norm": 5.910062789916992,
      "learning_rate": 0.0001022,
      "loss": 54.4406,
      "step": 490
    },
    {
      "epoch": 0.030888984987953297,
      "grad_norm": 5.083706378936768,
      "learning_rate": 0.00010020000000000001,
      "loss": 53.9375,
      "step": 500
    },
    {
      "epoch": 0.03150676468771236,
      "grad_norm": 6.0300188064575195,
      "learning_rate": 9.82e-05,
      "loss": 54.1531,
      "step": 510
    },
    {
      "epoch": 0.03212454438747143,
      "grad_norm": 5.980690002441406,
      "learning_rate": 9.620000000000001e-05,
      "loss": 54.2094,
      "step": 520
    },
    {
      "epoch": 0.0327423240872305,
      "grad_norm": 7.380679607391357,
      "learning_rate": 9.42e-05,
      "loss": 52.7875,
      "step": 530
    },
    {
      "epoch": 0.03336010378698956,
      "grad_norm": 5.659038066864014,
      "learning_rate": 9.22e-05,
      "loss": 55.7781,
      "step": 540
    },
    {
      "epoch": 0.03397788348674863,
      "grad_norm": 5.11006498336792,
      "learning_rate": 9.020000000000001e-05,
      "loss": 53.9906,
      "step": 550
    },
    {
      "epoch": 0.03459566318650769,
      "grad_norm": 6.410091876983643,
      "learning_rate": 8.82e-05,
      "loss": 54.7594,
      "step": 560
    },
    {
      "epoch": 0.03521344288626676,
      "grad_norm": 6.793738842010498,
      "learning_rate": 8.620000000000001e-05,
      "loss": 53.9438,
      "step": 570
    },
    {
      "epoch": 0.03583122258602582,
      "grad_norm": 6.46224308013916,
      "learning_rate": 8.42e-05,
      "loss": 53.1437,
      "step": 580
    },
    {
      "epoch": 0.03644900228578489,
      "grad_norm": 6.090053558349609,
      "learning_rate": 8.22e-05,
      "loss": 53.8344,
      "step": 590
    },
    {
      "epoch": 0.03706678198554395,
      "grad_norm": 6.247076988220215,
      "learning_rate": 8.020000000000001e-05,
      "loss": 54.5375,
      "step": 600
    },
    {
      "epoch": 0.03768456168530302,
      "grad_norm": 6.007680416107178,
      "learning_rate": 7.82e-05,
      "loss": 54.625,
      "step": 610
    },
    {
      "epoch": 0.03830234138506209,
      "grad_norm": 7.2675652503967285,
      "learning_rate": 7.620000000000001e-05,
      "loss": 54.8156,
      "step": 620
    },
    {
      "epoch": 0.03892012108482115,
      "grad_norm": 6.314589500427246,
      "learning_rate": 7.42e-05,
      "loss": 55.3125,
      "step": 630
    },
    {
      "epoch": 0.03953790078458022,
      "grad_norm": 6.402695178985596,
      "learning_rate": 7.22e-05,
      "loss": 53.8563,
      "step": 640
    },
    {
      "epoch": 0.04015568048433928,
      "grad_norm": 5.799248695373535,
      "learning_rate": 7.02e-05,
      "loss": 54.8469,
      "step": 650
    },
    {
      "epoch": 0.04077346018409835,
      "grad_norm": 7.128376007080078,
      "learning_rate": 6.82e-05,
      "loss": 53.775,
      "step": 660
    },
    {
      "epoch": 0.041391239883857414,
      "grad_norm": 7.135619640350342,
      "learning_rate": 6.620000000000001e-05,
      "loss": 54.5938,
      "step": 670
    },
    {
      "epoch": 0.04200901958361648,
      "grad_norm": 6.504344463348389,
      "learning_rate": 6.42e-05,
      "loss": 54.4813,
      "step": 680
    },
    {
      "epoch": 0.042626799283375545,
      "grad_norm": 5.756053447723389,
      "learning_rate": 6.220000000000001e-05,
      "loss": 54.7656,
      "step": 690
    },
    {
      "epoch": 0.043244578983134614,
      "grad_norm": 5.7243876457214355,
      "learning_rate": 6.02e-05,
      "loss": 54.7281,
      "step": 700
    },
    {
      "epoch": 0.04386235868289368,
      "grad_norm": 7.28043270111084,
      "learning_rate": 5.82e-05,
      "loss": 54.1125,
      "step": 710
    },
    {
      "epoch": 0.044480138382652745,
      "grad_norm": 7.261884689331055,
      "learning_rate": 5.620000000000001e-05,
      "loss": 54.2594,
      "step": 720
    },
    {
      "epoch": 0.045097918082411814,
      "grad_norm": 6.072547435760498,
      "learning_rate": 5.420000000000001e-05,
      "loss": 53.4281,
      "step": 730
    },
    {
      "epoch": 0.045715697782170876,
      "grad_norm": 8.60190486907959,
      "learning_rate": 5.22e-05,
      "loss": 53.6094,
      "step": 740
    },
    {
      "epoch": 0.046333477481929945,
      "grad_norm": 6.461378574371338,
      "learning_rate": 5.02e-05,
      "loss": 54.8625,
      "step": 750
    },
    {
      "epoch": 0.04695125718168901,
      "grad_norm": 5.880366802215576,
      "learning_rate": 4.82e-05,
      "loss": 53.2594,
      "step": 760
    },
    {
      "epoch": 0.047569036881448076,
      "grad_norm": 6.134253025054932,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 54.1625,
      "step": 770
    },
    {
      "epoch": 0.04818681658120714,
      "grad_norm": 5.9355058670043945,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 54.1406,
      "step": 780
    },
    {
      "epoch": 0.04880459628096621,
      "grad_norm": 5.849767684936523,
      "learning_rate": 4.22e-05,
      "loss": 53.5656,
      "step": 790
    },
    {
      "epoch": 0.049422375980725276,
      "grad_norm": 6.648584842681885,
      "learning_rate": 4.02e-05,
      "loss": 52.1313,
      "step": 800
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0541611390464e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
