{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.061777969975906594,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000617779699759066,
      "grad_norm": 10.763745307922363,
      "learning_rate": 1.982e-05,
      "loss": 67.6375,
      "step": 10
    },
    {
      "epoch": 0.001235559399518132,
      "grad_norm": 14.156059265136719,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 67.6281,
      "step": 20
    },
    {
      "epoch": 0.0018533390992771976,
      "grad_norm": 16.64623260498047,
      "learning_rate": 1.942e-05,
      "loss": 66.675,
      "step": 30
    },
    {
      "epoch": 0.002471118799036264,
      "grad_norm": 9.130790710449219,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 66.6562,
      "step": 40
    },
    {
      "epoch": 0.0030888984987953298,
      "grad_norm": 9.232744216918945,
      "learning_rate": 1.902e-05,
      "loss": 66.3937,
      "step": 50
    },
    {
      "epoch": 0.0037066781985543953,
      "grad_norm": 7.668697834014893,
      "learning_rate": 1.882e-05,
      "loss": 66.4719,
      "step": 60
    },
    {
      "epoch": 0.004324457898313461,
      "grad_norm": 8.529253959655762,
      "learning_rate": 1.862e-05,
      "loss": 66.075,
      "step": 70
    },
    {
      "epoch": 0.004942237598072528,
      "grad_norm": 8.245491981506348,
      "learning_rate": 1.8420000000000003e-05,
      "loss": 64.5406,
      "step": 80
    },
    {
      "epoch": 0.005560017297831593,
      "grad_norm": 7.166391849517822,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 65.0469,
      "step": 90
    },
    {
      "epoch": 0.0061777969975906595,
      "grad_norm": 6.951850891113281,
      "learning_rate": 1.802e-05,
      "loss": 64.65,
      "step": 100
    },
    {
      "epoch": 0.006795576697349725,
      "grad_norm": 8.10236930847168,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 64.925,
      "step": 110
    },
    {
      "epoch": 0.007413356397108791,
      "grad_norm": 6.500637531280518,
      "learning_rate": 1.762e-05,
      "loss": 64.5094,
      "step": 120
    },
    {
      "epoch": 0.008031136096867857,
      "grad_norm": 7.377358436584473,
      "learning_rate": 1.7420000000000003e-05,
      "loss": 63.8188,
      "step": 130
    },
    {
      "epoch": 0.008648915796626922,
      "grad_norm": 8.122247695922852,
      "learning_rate": 1.722e-05,
      "loss": 64.6906,
      "step": 140
    },
    {
      "epoch": 0.009266695496385988,
      "grad_norm": 7.850346088409424,
      "learning_rate": 1.702e-05,
      "loss": 64.7375,
      "step": 150
    },
    {
      "epoch": 0.009884475196145055,
      "grad_norm": 7.320540428161621,
      "learning_rate": 1.682e-05,
      "loss": 64.6312,
      "step": 160
    },
    {
      "epoch": 0.01050225489590412,
      "grad_norm": 7.235318183898926,
      "learning_rate": 1.662e-05,
      "loss": 63.7938,
      "step": 170
    },
    {
      "epoch": 0.011120034595663186,
      "grad_norm": 6.454173564910889,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 63.2406,
      "step": 180
    },
    {
      "epoch": 0.011737814295422252,
      "grad_norm": 6.765827178955078,
      "learning_rate": 1.6220000000000004e-05,
      "loss": 63.8969,
      "step": 190
    },
    {
      "epoch": 0.012355593995181319,
      "grad_norm": 6.890111923217773,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 62.0688,
      "step": 200
    },
    {
      "epoch": 0.012973373694940385,
      "grad_norm": 6.145503044128418,
      "learning_rate": 1.582e-05,
      "loss": 62.9813,
      "step": 210
    },
    {
      "epoch": 0.01359115339469945,
      "grad_norm": 7.040133953094482,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 62.6281,
      "step": 220
    },
    {
      "epoch": 0.014208933094458516,
      "grad_norm": 5.7061052322387695,
      "learning_rate": 1.542e-05,
      "loss": 62.0812,
      "step": 230
    },
    {
      "epoch": 0.014826712794217581,
      "grad_norm": 6.5179924964904785,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 62.2625,
      "step": 240
    },
    {
      "epoch": 0.015444492493976648,
      "grad_norm": 6.610644817352295,
      "learning_rate": 1.5020000000000002e-05,
      "loss": 62.4156,
      "step": 250
    },
    {
      "epoch": 0.016062272193735714,
      "grad_norm": 6.710115432739258,
      "learning_rate": 1.482e-05,
      "loss": 62.75,
      "step": 260
    },
    {
      "epoch": 0.01668005189349478,
      "grad_norm": 6.962050914764404,
      "learning_rate": 1.462e-05,
      "loss": 61.6375,
      "step": 270
    },
    {
      "epoch": 0.017297831593253845,
      "grad_norm": 6.520082950592041,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 62.4875,
      "step": 280
    },
    {
      "epoch": 0.01791561129301291,
      "grad_norm": 5.747843265533447,
      "learning_rate": 1.4220000000000001e-05,
      "loss": 62.9625,
      "step": 290
    },
    {
      "epoch": 0.018533390992771976,
      "grad_norm": 6.482231140136719,
      "learning_rate": 1.402e-05,
      "loss": 62.5844,
      "step": 300
    },
    {
      "epoch": 0.019151170692531045,
      "grad_norm": 6.510775089263916,
      "learning_rate": 1.382e-05,
      "loss": 62.4375,
      "step": 310
    },
    {
      "epoch": 0.01976895039229011,
      "grad_norm": 6.550113677978516,
      "learning_rate": 1.3620000000000002e-05,
      "loss": 63.2,
      "step": 320
    },
    {
      "epoch": 0.020386730092049176,
      "grad_norm": 7.0000481605529785,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 62.3937,
      "step": 330
    },
    {
      "epoch": 0.02100450979180824,
      "grad_norm": 7.770272731781006,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 61.7,
      "step": 340
    },
    {
      "epoch": 0.021622289491567307,
      "grad_norm": 5.494514465332031,
      "learning_rate": 1.302e-05,
      "loss": 61.9969,
      "step": 350
    },
    {
      "epoch": 0.022240069191326373,
      "grad_norm": 6.121674060821533,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 61.3875,
      "step": 360
    },
    {
      "epoch": 0.022857848891085438,
      "grad_norm": 6.578083038330078,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 61.2406,
      "step": 370
    },
    {
      "epoch": 0.023475628590844504,
      "grad_norm": 4.847102642059326,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 61.5812,
      "step": 380
    },
    {
      "epoch": 0.02409340829060357,
      "grad_norm": 6.130834579467773,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 61.0719,
      "step": 390
    },
    {
      "epoch": 0.024711187990362638,
      "grad_norm": 5.474390506744385,
      "learning_rate": 1.202e-05,
      "loss": 61.7031,
      "step": 400
    },
    {
      "epoch": 0.025328967690121704,
      "grad_norm": 5.524412155151367,
      "learning_rate": 1.182e-05,
      "loss": 60.3813,
      "step": 410
    },
    {
      "epoch": 0.02594674738988077,
      "grad_norm": 5.69834041595459,
      "learning_rate": 1.162e-05,
      "loss": 61.2563,
      "step": 420
    },
    {
      "epoch": 0.026564527089639835,
      "grad_norm": 5.14959192276001,
      "learning_rate": 1.142e-05,
      "loss": 61.3344,
      "step": 430
    },
    {
      "epoch": 0.0271823067893989,
      "grad_norm": 5.859739780426025,
      "learning_rate": 1.1220000000000003e-05,
      "loss": 61.6781,
      "step": 440
    },
    {
      "epoch": 0.027800086489157966,
      "grad_norm": 5.785032749176025,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 61.5094,
      "step": 450
    },
    {
      "epoch": 0.02841786618891703,
      "grad_norm": 5.104998588562012,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 61.3156,
      "step": 460
    },
    {
      "epoch": 0.029035645888676097,
      "grad_norm": 5.458083629608154,
      "learning_rate": 1.0620000000000002e-05,
      "loss": 61.4156,
      "step": 470
    },
    {
      "epoch": 0.029653425588435162,
      "grad_norm": 5.248126983642578,
      "learning_rate": 1.0420000000000002e-05,
      "loss": 62.2031,
      "step": 480
    },
    {
      "epoch": 0.03027120528819423,
      "grad_norm": 6.125324249267578,
      "learning_rate": 1.022e-05,
      "loss": 61.2469,
      "step": 490
    },
    {
      "epoch": 0.030888984987953297,
      "grad_norm": 4.866080284118652,
      "learning_rate": 1.002e-05,
      "loss": 61.1,
      "step": 500
    },
    {
      "epoch": 0.03150676468771236,
      "grad_norm": 5.825621128082275,
      "learning_rate": 9.820000000000001e-06,
      "loss": 61.1031,
      "step": 510
    },
    {
      "epoch": 0.03212454438747143,
      "grad_norm": 6.108590602874756,
      "learning_rate": 9.620000000000001e-06,
      "loss": 61.1375,
      "step": 520
    },
    {
      "epoch": 0.0327423240872305,
      "grad_norm": 7.722928524017334,
      "learning_rate": 9.42e-06,
      "loss": 60.05,
      "step": 530
    },
    {
      "epoch": 0.03336010378698956,
      "grad_norm": 5.8089423179626465,
      "learning_rate": 9.220000000000002e-06,
      "loss": 62.4937,
      "step": 540
    },
    {
      "epoch": 0.03397788348674863,
      "grad_norm": 6.153094291687012,
      "learning_rate": 9.020000000000002e-06,
      "loss": 60.8687,
      "step": 550
    },
    {
      "epoch": 0.03459566318650769,
      "grad_norm": 7.086178302764893,
      "learning_rate": 8.82e-06,
      "loss": 61.15,
      "step": 560
    },
    {
      "epoch": 0.03521344288626676,
      "grad_norm": 7.713687896728516,
      "learning_rate": 8.62e-06,
      "loss": 61.0094,
      "step": 570
    },
    {
      "epoch": 0.03583122258602582,
      "grad_norm": 5.968174934387207,
      "learning_rate": 8.42e-06,
      "loss": 60.3,
      "step": 580
    },
    {
      "epoch": 0.03644900228578489,
      "grad_norm": 5.530824661254883,
      "learning_rate": 8.220000000000001e-06,
      "loss": 61.1688,
      "step": 590
    },
    {
      "epoch": 0.03706678198554395,
      "grad_norm": 5.949508190155029,
      "learning_rate": 8.020000000000001e-06,
      "loss": 61.2031,
      "step": 600
    },
    {
      "epoch": 0.03768456168530302,
      "grad_norm": 5.263463497161865,
      "learning_rate": 7.820000000000001e-06,
      "loss": 61.4906,
      "step": 610
    },
    {
      "epoch": 0.03830234138506209,
      "grad_norm": 6.112362861633301,
      "learning_rate": 7.620000000000001e-06,
      "loss": 61.875,
      "step": 620
    },
    {
      "epoch": 0.03892012108482115,
      "grad_norm": 6.310627460479736,
      "learning_rate": 7.420000000000001e-06,
      "loss": 62.1313,
      "step": 630
    },
    {
      "epoch": 0.03953790078458022,
      "grad_norm": 5.944725036621094,
      "learning_rate": 7.22e-06,
      "loss": 60.625,
      "step": 640
    },
    {
      "epoch": 0.04015568048433928,
      "grad_norm": 4.971569538116455,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 62.1594,
      "step": 650
    },
    {
      "epoch": 0.04077346018409835,
      "grad_norm": 5.674368858337402,
      "learning_rate": 6.820000000000001e-06,
      "loss": 60.6313,
      "step": 660
    },
    {
      "epoch": 0.041391239883857414,
      "grad_norm": 6.959671497344971,
      "learning_rate": 6.620000000000001e-06,
      "loss": 61.35,
      "step": 670
    },
    {
      "epoch": 0.04200901958361648,
      "grad_norm": 5.745963096618652,
      "learning_rate": 6.42e-06,
      "loss": 61.4719,
      "step": 680
    },
    {
      "epoch": 0.042626799283375545,
      "grad_norm": 4.666982650756836,
      "learning_rate": 6.220000000000001e-06,
      "loss": 61.6437,
      "step": 690
    },
    {
      "epoch": 0.043244578983134614,
      "grad_norm": 5.3082685470581055,
      "learning_rate": 6.02e-06,
      "loss": 61.6938,
      "step": 700
    },
    {
      "epoch": 0.04386235868289368,
      "grad_norm": 7.026266574859619,
      "learning_rate": 5.82e-06,
      "loss": 61.2563,
      "step": 710
    },
    {
      "epoch": 0.044480138382652745,
      "grad_norm": 5.772981643676758,
      "learning_rate": 5.620000000000001e-06,
      "loss": 61.0844,
      "step": 720
    },
    {
      "epoch": 0.045097918082411814,
      "grad_norm": 5.861734390258789,
      "learning_rate": 5.420000000000001e-06,
      "loss": 60.3844,
      "step": 730
    },
    {
      "epoch": 0.045715697782170876,
      "grad_norm": 6.777566432952881,
      "learning_rate": 5.220000000000001e-06,
      "loss": 60.7062,
      "step": 740
    },
    {
      "epoch": 0.046333477481929945,
      "grad_norm": 5.702578067779541,
      "learning_rate": 5.02e-06,
      "loss": 61.8594,
      "step": 750
    },
    {
      "epoch": 0.04695125718168901,
      "grad_norm": 5.598859786987305,
      "learning_rate": 4.8200000000000004e-06,
      "loss": 60.6219,
      "step": 760
    },
    {
      "epoch": 0.047569036881448076,
      "grad_norm": 6.205381393432617,
      "learning_rate": 4.620000000000001e-06,
      "loss": 60.9688,
      "step": 770
    },
    {
      "epoch": 0.04818681658120714,
      "grad_norm": 6.0221171379089355,
      "learning_rate": 4.42e-06,
      "loss": 61.0656,
      "step": 780
    },
    {
      "epoch": 0.04880459628096621,
      "grad_norm": 5.6937336921691895,
      "learning_rate": 4.22e-06,
      "loss": 60.5156,
      "step": 790
    },
    {
      "epoch": 0.049422375980725276,
      "grad_norm": 6.804581642150879,
      "learning_rate": 4.0200000000000005e-06,
      "loss": 59.7313,
      "step": 800
    },
    {
      "epoch": 0.05004015568048434,
      "grad_norm": 5.632901191711426,
      "learning_rate": 3.820000000000001e-06,
      "loss": 60.2719,
      "step": 810
    },
    {
      "epoch": 0.05065793538024341,
      "grad_norm": 6.304638862609863,
      "learning_rate": 3.62e-06,
      "loss": 62.0906,
      "step": 820
    },
    {
      "epoch": 0.05127571508000247,
      "grad_norm": 6.253172397613525,
      "learning_rate": 3.4200000000000007e-06,
      "loss": 60.8969,
      "step": 830
    },
    {
      "epoch": 0.05189349477976154,
      "grad_norm": 5.1264472007751465,
      "learning_rate": 3.2200000000000005e-06,
      "loss": 60.2656,
      "step": 840
    },
    {
      "epoch": 0.0525112744795206,
      "grad_norm": 6.53247594833374,
      "learning_rate": 3.0200000000000003e-06,
      "loss": 61.3438,
      "step": 850
    },
    {
      "epoch": 0.05312905417927967,
      "grad_norm": 5.865685939788818,
      "learning_rate": 2.82e-06,
      "loss": 60.1156,
      "step": 860
    },
    {
      "epoch": 0.05374683387903873,
      "grad_norm": 6.108875274658203,
      "learning_rate": 2.6200000000000003e-06,
      "loss": 61.3969,
      "step": 870
    },
    {
      "epoch": 0.0543646135787978,
      "grad_norm": 5.031070709228516,
      "learning_rate": 2.42e-06,
      "loss": 60.6031,
      "step": 880
    },
    {
      "epoch": 0.05498239327855687,
      "grad_norm": 5.895191192626953,
      "learning_rate": 2.2200000000000003e-06,
      "loss": 60.8687,
      "step": 890
    },
    {
      "epoch": 0.05560017297831593,
      "grad_norm": 6.493716239929199,
      "learning_rate": 2.02e-06,
      "loss": 60.5187,
      "step": 900
    },
    {
      "epoch": 0.056217952678075,
      "grad_norm": 5.648416996002197,
      "learning_rate": 1.8200000000000002e-06,
      "loss": 60.7281,
      "step": 910
    },
    {
      "epoch": 0.05683573237783406,
      "grad_norm": 5.9306321144104,
      "learning_rate": 1.6200000000000002e-06,
      "loss": 61.2344,
      "step": 920
    },
    {
      "epoch": 0.05745351207759313,
      "grad_norm": 5.786369323730469,
      "learning_rate": 1.42e-06,
      "loss": 60.6625,
      "step": 930
    },
    {
      "epoch": 0.058071291777352194,
      "grad_norm": 5.899538516998291,
      "learning_rate": 1.2200000000000002e-06,
      "loss": 61.4281,
      "step": 940
    },
    {
      "epoch": 0.05868907147711126,
      "grad_norm": 5.4771318435668945,
      "learning_rate": 1.02e-06,
      "loss": 61.2906,
      "step": 950
    },
    {
      "epoch": 0.059306851176870325,
      "grad_norm": 8.279982566833496,
      "learning_rate": 8.200000000000001e-07,
      "loss": 61.2125,
      "step": 960
    },
    {
      "epoch": 0.059924630876629394,
      "grad_norm": 6.301840782165527,
      "learning_rate": 6.200000000000001e-07,
      "loss": 61.2219,
      "step": 970
    },
    {
      "epoch": 0.06054241057638846,
      "grad_norm": 4.898102760314941,
      "learning_rate": 4.2000000000000006e-07,
      "loss": 60.8312,
      "step": 980
    },
    {
      "epoch": 0.061160190276147525,
      "grad_norm": 6.433183193206787,
      "learning_rate": 2.2e-07,
      "loss": 60.6187,
      "step": 990
    },
    {
      "epoch": 0.061777969975906594,
      "grad_norm": 5.335325241088867,
      "learning_rate": 2e-08,
      "loss": 61.1844,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2892631072372736e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
