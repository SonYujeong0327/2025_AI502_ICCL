{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.049422375980725276,
  "eval_steps": 500,
  "global_step": 800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000617779699759066,
      "grad_norm": 10.004377365112305,
      "learning_rate": 0.00019820000000000002,
      "loss": 64.9938,
      "step": 10
    },
    {
      "epoch": 0.001235559399518132,
      "grad_norm": 8.78490161895752,
      "learning_rate": 0.0001962,
      "loss": 62.3594,
      "step": 20
    },
    {
      "epoch": 0.0018533390992771976,
      "grad_norm": 7.871920585632324,
      "learning_rate": 0.0001942,
      "loss": 60.9094,
      "step": 30
    },
    {
      "epoch": 0.002471118799036264,
      "grad_norm": 7.973700523376465,
      "learning_rate": 0.0001922,
      "loss": 60.5969,
      "step": 40
    },
    {
      "epoch": 0.0030888984987953298,
      "grad_norm": 8.244644165039062,
      "learning_rate": 0.0001902,
      "loss": 60.425,
      "step": 50
    },
    {
      "epoch": 0.0037066781985543953,
      "grad_norm": 8.27055835723877,
      "learning_rate": 0.0001882,
      "loss": 60.5312,
      "step": 60
    },
    {
      "epoch": 0.004324457898313461,
      "grad_norm": 7.371124267578125,
      "learning_rate": 0.00018620000000000003,
      "loss": 59.775,
      "step": 70
    },
    {
      "epoch": 0.004942237598072528,
      "grad_norm": 7.712457180023193,
      "learning_rate": 0.0001842,
      "loss": 58.3844,
      "step": 80
    },
    {
      "epoch": 0.005560017297831593,
      "grad_norm": 7.062561511993408,
      "learning_rate": 0.0001822,
      "loss": 58.9062,
      "step": 90
    },
    {
      "epoch": 0.0061777969975906595,
      "grad_norm": 8.073551177978516,
      "learning_rate": 0.00018020000000000002,
      "loss": 58.5938,
      "step": 100
    },
    {
      "epoch": 0.006795576697349725,
      "grad_norm": 9.758410453796387,
      "learning_rate": 0.00017820000000000002,
      "loss": 58.8344,
      "step": 110
    },
    {
      "epoch": 0.007413356397108791,
      "grad_norm": 8.886861801147461,
      "learning_rate": 0.0001762,
      "loss": 58.7125,
      "step": 120
    },
    {
      "epoch": 0.008031136096867857,
      "grad_norm": 8.399168014526367,
      "learning_rate": 0.0001742,
      "loss": 57.975,
      "step": 130
    },
    {
      "epoch": 0.008648915796626922,
      "grad_norm": 10.718302726745605,
      "learning_rate": 0.0001722,
      "loss": 59.1625,
      "step": 140
    },
    {
      "epoch": 0.009266695496385988,
      "grad_norm": 9.303407669067383,
      "learning_rate": 0.00017020000000000002,
      "loss": 59.2844,
      "step": 150
    },
    {
      "epoch": 0.009884475196145055,
      "grad_norm": 8.42739486694336,
      "learning_rate": 0.0001682,
      "loss": 59.5219,
      "step": 160
    },
    {
      "epoch": 0.01050225489590412,
      "grad_norm": 9.445713996887207,
      "learning_rate": 0.0001662,
      "loss": 58.8937,
      "step": 170
    },
    {
      "epoch": 0.011120034595663186,
      "grad_norm": 8.022174835205078,
      "learning_rate": 0.0001642,
      "loss": 58.4438,
      "step": 180
    },
    {
      "epoch": 0.011737814295422252,
      "grad_norm": 8.688960075378418,
      "learning_rate": 0.0001622,
      "loss": 59.1063,
      "step": 190
    },
    {
      "epoch": 0.012355593995181319,
      "grad_norm": 7.8960161209106445,
      "learning_rate": 0.00016020000000000002,
      "loss": 57.0469,
      "step": 200
    },
    {
      "epoch": 0.012973373694940385,
      "grad_norm": 7.836530685424805,
      "learning_rate": 0.00015820000000000002,
      "loss": 58.2031,
      "step": 210
    },
    {
      "epoch": 0.01359115339469945,
      "grad_norm": 9.732193946838379,
      "learning_rate": 0.0001562,
      "loss": 58.0063,
      "step": 220
    },
    {
      "epoch": 0.014208933094458516,
      "grad_norm": 7.454075813293457,
      "learning_rate": 0.0001542,
      "loss": 57.7125,
      "step": 230
    },
    {
      "epoch": 0.014826712794217581,
      "grad_norm": 9.21840763092041,
      "learning_rate": 0.0001522,
      "loss": 57.8625,
      "step": 240
    },
    {
      "epoch": 0.015444492493976648,
      "grad_norm": 8.942570686340332,
      "learning_rate": 0.00015020000000000002,
      "loss": 57.7563,
      "step": 250
    },
    {
      "epoch": 0.016062272193735714,
      "grad_norm": 8.564427375793457,
      "learning_rate": 0.0001482,
      "loss": 58.0438,
      "step": 260
    },
    {
      "epoch": 0.01668005189349478,
      "grad_norm": 9.700596809387207,
      "learning_rate": 0.0001462,
      "loss": 57.1125,
      "step": 270
    },
    {
      "epoch": 0.017297831593253845,
      "grad_norm": 9.209318161010742,
      "learning_rate": 0.0001442,
      "loss": 58.2,
      "step": 280
    },
    {
      "epoch": 0.01791561129301291,
      "grad_norm": 8.28193473815918,
      "learning_rate": 0.0001422,
      "loss": 58.8563,
      "step": 290
    },
    {
      "epoch": 0.018533390992771976,
      "grad_norm": 8.750894546508789,
      "learning_rate": 0.0001402,
      "loss": 58.2219,
      "step": 300
    },
    {
      "epoch": 0.019151170692531045,
      "grad_norm": 10.136998176574707,
      "learning_rate": 0.0001382,
      "loss": 58.5094,
      "step": 310
    },
    {
      "epoch": 0.01976895039229011,
      "grad_norm": 8.08810806274414,
      "learning_rate": 0.0001362,
      "loss": 59.1719,
      "step": 320
    },
    {
      "epoch": 0.020386730092049176,
      "grad_norm": 9.663783073425293,
      "learning_rate": 0.0001342,
      "loss": 58.175,
      "step": 330
    },
    {
      "epoch": 0.02100450979180824,
      "grad_norm": 13.774109840393066,
      "learning_rate": 0.00013220000000000001,
      "loss": 57.7344,
      "step": 340
    },
    {
      "epoch": 0.021622289491567307,
      "grad_norm": 9.13471794128418,
      "learning_rate": 0.00013020000000000002,
      "loss": 57.8062,
      "step": 350
    },
    {
      "epoch": 0.022240069191326373,
      "grad_norm": 8.008355140686035,
      "learning_rate": 0.0001282,
      "loss": 57.4813,
      "step": 360
    },
    {
      "epoch": 0.022857848891085438,
      "grad_norm": 8.63047981262207,
      "learning_rate": 0.0001262,
      "loss": 56.8844,
      "step": 370
    },
    {
      "epoch": 0.023475628590844504,
      "grad_norm": 8.046939849853516,
      "learning_rate": 0.0001242,
      "loss": 58.0344,
      "step": 380
    },
    {
      "epoch": 0.02409340829060357,
      "grad_norm": 9.64215087890625,
      "learning_rate": 0.00012220000000000002,
      "loss": 57.0094,
      "step": 390
    },
    {
      "epoch": 0.024711187990362638,
      "grad_norm": 9.062531471252441,
      "learning_rate": 0.00012020000000000001,
      "loss": 57.8906,
      "step": 400
    },
    {
      "epoch": 0.025328967690121704,
      "grad_norm": 9.134039878845215,
      "learning_rate": 0.0001182,
      "loss": 56.7531,
      "step": 410
    },
    {
      "epoch": 0.02594674738988077,
      "grad_norm": 10.036556243896484,
      "learning_rate": 0.00011619999999999999,
      "loss": 57.6875,
      "step": 420
    },
    {
      "epoch": 0.026564527089639835,
      "grad_norm": 8.592611312866211,
      "learning_rate": 0.0001142,
      "loss": 57.3937,
      "step": 430
    },
    {
      "epoch": 0.0271823067893989,
      "grad_norm": 9.322001457214355,
      "learning_rate": 0.00011220000000000002,
      "loss": 58.2281,
      "step": 440
    },
    {
      "epoch": 0.027800086489157966,
      "grad_norm": 10.247467041015625,
      "learning_rate": 0.00011020000000000001,
      "loss": 57.6125,
      "step": 450
    },
    {
      "epoch": 0.02841786618891703,
      "grad_norm": 8.067656517028809,
      "learning_rate": 0.00010820000000000001,
      "loss": 57.375,
      "step": 460
    },
    {
      "epoch": 0.029035645888676097,
      "grad_norm": 9.694336891174316,
      "learning_rate": 0.0001062,
      "loss": 57.6031,
      "step": 470
    },
    {
      "epoch": 0.029653425588435162,
      "grad_norm": 8.672944068908691,
      "learning_rate": 0.00010420000000000001,
      "loss": 58.6875,
      "step": 480
    },
    {
      "epoch": 0.03027120528819423,
      "grad_norm": 8.99588394165039,
      "learning_rate": 0.0001022,
      "loss": 57.5781,
      "step": 490
    },
    {
      "epoch": 0.030888984987953297,
      "grad_norm": 7.980093955993652,
      "learning_rate": 0.00010020000000000001,
      "loss": 57.1094,
      "step": 500
    },
    {
      "epoch": 0.03150676468771236,
      "grad_norm": 9.072317123413086,
      "learning_rate": 9.82e-05,
      "loss": 57.3594,
      "step": 510
    },
    {
      "epoch": 0.03212454438747143,
      "grad_norm": 9.823476791381836,
      "learning_rate": 9.620000000000001e-05,
      "loss": 57.4062,
      "step": 520
    },
    {
      "epoch": 0.0327423240872305,
      "grad_norm": 11.523143768310547,
      "learning_rate": 9.42e-05,
      "loss": 56.25,
      "step": 530
    },
    {
      "epoch": 0.03336010378698956,
      "grad_norm": 8.677119255065918,
      "learning_rate": 9.22e-05,
      "loss": 59.0906,
      "step": 540
    },
    {
      "epoch": 0.03397788348674863,
      "grad_norm": 7.332343578338623,
      "learning_rate": 9.020000000000001e-05,
      "loss": 57.4562,
      "step": 550
    },
    {
      "epoch": 0.03459566318650769,
      "grad_norm": 10.2174072265625,
      "learning_rate": 8.82e-05,
      "loss": 57.9062,
      "step": 560
    },
    {
      "epoch": 0.03521344288626676,
      "grad_norm": 10.521509170532227,
      "learning_rate": 8.620000000000001e-05,
      "loss": 57.2531,
      "step": 570
    },
    {
      "epoch": 0.03583122258602582,
      "grad_norm": 9.419682502746582,
      "learning_rate": 8.42e-05,
      "loss": 56.6125,
      "step": 580
    },
    {
      "epoch": 0.03644900228578489,
      "grad_norm": 9.069539070129395,
      "learning_rate": 8.22e-05,
      "loss": 57.35,
      "step": 590
    },
    {
      "epoch": 0.03706678198554395,
      "grad_norm": 9.394938468933105,
      "learning_rate": 8.020000000000001e-05,
      "loss": 57.6063,
      "step": 600
    },
    {
      "epoch": 0.03768456168530302,
      "grad_norm": 9.236974716186523,
      "learning_rate": 7.82e-05,
      "loss": 57.8906,
      "step": 610
    },
    {
      "epoch": 0.03830234138506209,
      "grad_norm": 10.865123748779297,
      "learning_rate": 7.620000000000001e-05,
      "loss": 58.1125,
      "step": 620
    },
    {
      "epoch": 0.03892012108482115,
      "grad_norm": 10.194205284118652,
      "learning_rate": 7.42e-05,
      "loss": 58.6125,
      "step": 630
    },
    {
      "epoch": 0.03953790078458022,
      "grad_norm": 9.938148498535156,
      "learning_rate": 7.22e-05,
      "loss": 57.0844,
      "step": 640
    },
    {
      "epoch": 0.04015568048433928,
      "grad_norm": 8.665750503540039,
      "learning_rate": 7.02e-05,
      "loss": 58.2281,
      "step": 650
    },
    {
      "epoch": 0.04077346018409835,
      "grad_norm": 10.720528602600098,
      "learning_rate": 6.82e-05,
      "loss": 57.0,
      "step": 660
    },
    {
      "epoch": 0.041391239883857414,
      "grad_norm": 9.558297157287598,
      "learning_rate": 6.620000000000001e-05,
      "loss": 57.7563,
      "step": 670
    },
    {
      "epoch": 0.04200901958361648,
      "grad_norm": 9.880847930908203,
      "learning_rate": 6.42e-05,
      "loss": 57.7531,
      "step": 680
    },
    {
      "epoch": 0.042626799283375545,
      "grad_norm": 8.598581314086914,
      "learning_rate": 6.220000000000001e-05,
      "loss": 57.9656,
      "step": 690
    },
    {
      "epoch": 0.043244578983134614,
      "grad_norm": 9.076126098632812,
      "learning_rate": 6.02e-05,
      "loss": 57.9875,
      "step": 700
    },
    {
      "epoch": 0.04386235868289368,
      "grad_norm": 11.133062362670898,
      "learning_rate": 5.82e-05,
      "loss": 57.6656,
      "step": 710
    },
    {
      "epoch": 0.044480138382652745,
      "grad_norm": 10.494436264038086,
      "learning_rate": 5.620000000000001e-05,
      "loss": 57.5812,
      "step": 720
    },
    {
      "epoch": 0.045097918082411814,
      "grad_norm": 9.23215389251709,
      "learning_rate": 5.420000000000001e-05,
      "loss": 56.7625,
      "step": 730
    },
    {
      "epoch": 0.045715697782170876,
      "grad_norm": 10.859646797180176,
      "learning_rate": 5.22e-05,
      "loss": 57.125,
      "step": 740
    },
    {
      "epoch": 0.046333477481929945,
      "grad_norm": 9.05699634552002,
      "learning_rate": 5.02e-05,
      "loss": 58.175,
      "step": 750
    },
    {
      "epoch": 0.04695125718168901,
      "grad_norm": 9.348727226257324,
      "learning_rate": 4.82e-05,
      "loss": 56.6969,
      "step": 760
    },
    {
      "epoch": 0.047569036881448076,
      "grad_norm": 10.264200210571289,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 57.4,
      "step": 770
    },
    {
      "epoch": 0.04818681658120714,
      "grad_norm": 9.072318077087402,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 57.5187,
      "step": 780
    },
    {
      "epoch": 0.04880459628096621,
      "grad_norm": 8.84623908996582,
      "learning_rate": 4.22e-05,
      "loss": 56.9594,
      "step": 790
    },
    {
      "epoch": 0.049422375980725276,
      "grad_norm": 10.154704093933105,
      "learning_rate": 4.02e-05,
      "loss": 55.8406,
      "step": 800
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0541611390464e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
