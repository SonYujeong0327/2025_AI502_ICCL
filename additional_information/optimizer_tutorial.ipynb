{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "781ef965",
      "metadata": {},
      "source": [
        "# Optimizer 튜토리얼: AdamW와 Muon 비교\n",
        "\n",
        "이 노트북은 `models.default_model`의 소형 Transformer를 사용하여 옵티마이저 AdamW와 Muon의 학습 경향을 비교합니다. 학습 데이터는 `resource/fineweb_llama32_128k_tokens` 경로에 저장된 FineWeb 10BT 스트리밍 샘플을 메타 Llama 3.2 토크나이저로 전처리한 12.8만 토큰입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcd3730",
      "metadata": {},
      "source": [
        "## 실험 개요\n",
        "\n",
        "- 기본 모델: `TransformerForCausalLM` (eager self-attention, 2층, hidden size 128)\n",
        "- 데이터: FineWeb 10BT 스트리밍 샘플에서 추출한 128k 토큰 (`resource/fineweb_llama32_128k_tokens`)\n",
        "- 비교 옵티마이저: `torch.optim.AdamW` vs `models.example_optimizer.Muon`\n",
        "- 평가 지표: 학습 스텝마다 기록한 크로스엔트로피 손실"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77b10c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from models.default_model import TransformerForCausalLM, TransformerConfig\n",
        "from models.example_optimizer import Muon\n",
        "\n",
        "TOKEN_DATA_PATH = Path(\"resource/fineweb_llama32_128k_tokens\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용 중인 디바이스: {device}\")\n",
        "print(f\"토큰 데이터 경로: {TOKEN_DATA_PATH.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcd348f",
      "metadata": {},
      "source": [
        "## FineWeb 토큰 데이터 준비\n",
        "\n",
        "미리 스트리밍 방식으로 수집해둔 `resource/fineweb_llama32_128k_tokens` Arrow 데이터셋에는 Llama 3.2 토크나이저로 인코딩한 토큰 ID가 저장되어 있습니다. 이를 불러와 일정 길이의 시퀀스로 잘라 PyTorch 데이터셋을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6269ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenChunkDataset(Dataset):\n",
        "    def __init__(self, tokens: torch.Tensor, seq_len: int, vocab_size: int):\n",
        "        if tokens.ndim != 1:\n",
        "            tokens = tokens.view(-1)\n",
        "        usable = (tokens.numel() // seq_len) * seq_len\n",
        "        if usable == 0:\n",
        "            raise ValueError(\"토큰 수가 시퀀스 길이보다 작습니다.\")\n",
        "\n",
        "        self.tokens = tokens[:usable]\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_sequences = usable // seq_len\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        start = idx * self.seq_len\n",
        "        end = start + self.seq_len\n",
        "        return self.tokens[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9464c08e",
      "metadata": {},
      "outputs": [],
      "source": [
        "arrow_dataset = load_from_disk(str(TOKEN_DATA_PATH))\n",
        "flat_tokens = arrow_dataset[0][\"input_ids\"]\n",
        "token_tensor = torch.tensor(flat_tokens, dtype=torch.long)\n",
        "\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset = TokenChunkDataset(token_tensor, seq_len=SEQ_LEN, vocab_size=tokenizer.vocab_size)\n",
        "\n",
        "print(\n",
        "    f\"총 토큰 수: {token_tensor.numel():,}, 사용 가능한 토큰: {len(dataset) * SEQ_LEN:,}, \"\n",
        "    f\"시퀀스 수: {len(dataset)}, 시퀀스 길이: {SEQ_LEN}\"\n",
        ")\n",
        "print(f\"배치 크기: {BATCH_SIZE}, vocab_size: {dataset.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b05e663",
      "metadata": {},
      "source": [
        "## 기본 Transformer 구성\n",
        "\n",
        "`TransformerConfig`를 직접 생성해 작은 모델을 정의합니다. attention 구현은 기본값인 eager를 그대로 사용하고, 컨텍스트 길이는 128 토큰으로 제한합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef13800e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_small_config(vocab_size: int) -> TransformerConfig:\n",
        "    return TransformerConfig(\n",
        "        base_model_name_or_path=None,\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=128,\n",
        "        intermediate_size=256,\n",
        "        num_hidden_layers=2,\n",
        "        num_attention_heads=4,\n",
        "        num_key_value_heads=2,\n",
        "        max_position_embeddings=128,\n",
        "        attention_dropout=0.0,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0e9119",
      "metadata": {},
      "source": [
        "## 공통 학습 루프 정의\n",
        "\n",
        "두 옵티마이저가 동일한 초기 가중치에서 시작하도록 학습 전에 난수 시드를 고정합니다. 각 스텝마다 손실을 기록해 이후 그래프로 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8980e1cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_with_optimizer(optimizer_name: str, steps: int = 200) -> torch.Tensor:\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    config = build_small_config(vocab_size=dataset.vocab_size)\n",
        "    model = TransformerForCausalLM(config).to(device)\n",
        "    model.train()\n",
        "\n",
        "    if optimizer_name.lower() == \"adamw\":\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=2e-3,\n",
        "            betas=(0.9, 0.95),\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "    elif optimizer_name.lower() == \"muon\":\n",
        "        optimizer = Muon(\n",
        "            model.parameters(),\n",
        "            lr=2e-2,\n",
        "            momentum=0.95,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
        "\n",
        "    def create_loader() -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            generator=torch.Generator().manual_seed(2024),\n",
        "        )\n",
        "\n",
        "    data_loader = create_loader()\n",
        "    data_iter = iter(data_loader)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for step in range(1, steps + 1):\n",
        "        try:\n",
        "            batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_loader = create_loader()\n",
        "            data_iter = iter(data_loader)\n",
        "            batch = next(data_iter)\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        labels = batch.clone()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=batch, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return torch.tensor(losses, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffee299",
      "metadata": {},
      "source": [
        "## 실험 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3106850e",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_steps = 200\n",
        "\n",
        "adamw_losses = train_with_optimizer(\"adamw\", steps=total_steps)\n",
        "muon_losses = train_with_optimizer(\"muon\", steps=total_steps)\n",
        "\n",
        "comparison = {\n",
        "    \"AdamW\": adamw_losses,\n",
        "    \"Muon\": muon_losses,\n",
        "}\n",
        "\n",
        "for name, losses in comparison.items():\n",
        "    print(f\"{name:5s} | 초기 loss: {losses[0]:.3f} -> 최종 loss: {losses[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65b6efe",
      "metadata": {},
      "source": [
        "## 결과 시각화\n",
        "\n",
        "손실 곡선은 5 스텝 이동 평균으로 부드럽게 만들어 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e369fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def moving_average(values: torch.Tensor, window: int = 5) -> np.ndarray:\n",
        "    array = values.cpu().numpy()\n",
        "    if len(array) < window:\n",
        "        return array\n",
        "    kernel = np.ones(window, dtype=np.float32) / window\n",
        "    return np.convolve(array, kernel, mode=\"valid\")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "for name, losses in comparison.items():\n",
        "    smoothed = moving_average(losses, window=5)\n",
        "    plt.plot(\n",
        "        np.arange(len(smoothed)),\n",
        "        smoothed,\n",
        "        label=name,\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.ylabel(\"Loss (moving average)\")\n",
        "plt.title(\"AdamW vs Muon 학습 곡선\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e986e7ea",
      "metadata": {},
      "source": [
        "## 마무리\n",
        "\n",
        "- 이동 평균 손실 곡선을 통해 두 옵티마이저의 수렴 속도와 안정성을 직관적으로 비교할 수 있습니다.\n",
        "- Muon은 직교화 기반 업데이트를 사용하므로 행렬 파라미터가 많은 레이어에서 더 공격적인 학습률을 사용해도 안정적인 경향을 보입니다.\n",
        "- AdamW는 더 익숙한 기본 옵티마이저이므로, 기준선으로 활용하고 추가 실험에서 하이퍼파라미터를 조정해볼 수 있습니다.\n",
        "\n",
        "필요에 따라 `train_with_optimizer`의 학습 스텝 수나 하이퍼파라미터를 조정하며 다양한 시나리오를 실험해보세요."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}