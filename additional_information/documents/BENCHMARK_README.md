# Performance Benchmark Documentation

## ê°œìš”

`benchmark.py`ëŠ” default_modelê³¼ example_modelì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ì…ë‹ˆë‹¤.

## ì¸¡ì • ì§€í‘œ (Performance Metrics)

### 1. Wall Clock Time â±ï¸

ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

**Training Mode**
- Forward pass + Backward pass + Zero grad
- Mixed precision (bfloat16) ì‚¬ìš©
- í†µê³„: mean, median, std, p95, p99 (ms)

**Inference Mode**
- Forward passë§Œ
- `torch.no_grad()` ì‚¬ìš©
- í†µê³„: mean, median, std, p95, p99 (ms)

**Throughput**
- Samples per second
- Tokens per second

### 2. Memory Consumption ğŸ’¾

GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•©ë‹ˆë‹¤.

**ì¸¡ì • í•­ëª©**
- **Peak Memory**: ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
- **Current Memory**: í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬ (MB)
- **Reserved Memory**: GPUì— ì˜ˆì•½ëœ ì „ì²´ ë©”ëª¨ë¦¬ (MB)

**ì¶”ì  ë°©ë²•**
- `torch.cuda.memory_allocated()`: ì‹¤ì œ í• ë‹¹ëœ ë©”ëª¨ë¦¬
- `torch.cuda.max_memory_allocated()`: í”¼í¬ ë©”ëª¨ë¦¬
- `torch.cuda.memory_reserved()`: ìºì‹œ í¬í•¨ ì˜ˆì•½ ë©”ëª¨ë¦¬

### 3. Generation Latency ğŸš€

Autoregressive generationì˜ ì§€ì—°ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

**First Token Latency (Prefill)**
- ì „ì²´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹œê°„
- KV cache ì´ˆê¸° êµ¬ì¶•
- í†µê³„: mean, median, p95, p99 (ms)

**Per-token Latency (Decode)**
- ê° í† í° ìƒì„± ì‹œê°„
- KV cache í™œìš©í•œ incremental generation
- í†µê³„: mean, median, p95, p99 (ms)

**Throughput**
- Tokens per second (decode phase)

## Warmupì˜ ì¤‘ìš”ì„±

### ì™œ 10 step warmupì´ í•„ìš”í•œê°€?

#### 1. torch.compile ìµœì í™”
```python
model = torch.compile(model, mode="default")
```

**ì²« ì‹¤í–‰ ì‹œ:**
- CUDA graph ìƒì„± ë° ìµœì í™”
- Kernel fusion ìˆ˜í–‰
- ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ ìµœì í™”
- Dynamic shape handling í•™ìŠµ

**Warmup í›„:**
- ìµœì í™”ëœ CUDA graph ì¬ì‚¬ìš©
- ì•ˆì •ì ì¸ ì„±ëŠ¥ ì¸¡ì • ê°€ëŠ¥

#### 2. Triton Kernel ìµœì í™” (Liger Kernel)
```python
# Liger Kernel uses Triton
from liger_kernel.transformers.rms_norm import LigerRMSNorm
```

**Autotuning ê³¼ì •:**
- GPU ì•„í‚¤í…ì²˜ ê°ì§€ (A100, V100, etc.)
- Block size, thread êµ¬ì„± ìµœì í™”
- Shared memory ì‚¬ìš© íŒ¨í„´ í•™ìŠµ
- Kernel cache ìƒì„±

#### 3. Flash Attention 2 ìµœì í™”
```python
from flash_attn import flash_attn_func
```

**ìµœì í™” ìš”ì†Œ:**
- CUDA kernel ë¡œë”©
- Tile size ìµœì í™”
- Memory layout ì¡°ì •

#### 4. CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
- cuBLAS context ìƒì„±
- cuDNN algorithm selection
- Memory pool initialization

### Warmup í›„ ì„±ëŠ¥ ì°¨ì´

| Phase | First Run | After Warmup | Speedup |
|-------|-----------|--------------|---------|
| torch.compile | ~1000ms | ~50ms | 20x |
| Triton kernel | ~100ms | ~5ms | 20x |
| Flash Attention | ~50ms | ~3ms | 16x |

## ì‹¤í–‰ ë°©ë²•

### ê¸°ë³¸ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì„¤ì • (ê¶Œì¥)
python benchmark.py

# ê²°ê³¼:
# - benchmark_results.json ìƒì„±
# - ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥
```

### ìƒì„¸ ì„¤ì •

```bash
# ë°°ì¹˜ í¬ê¸° ë° ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •
python benchmark.py --batch-size 4 --seq-length 1024

# Warmup ë° ë²¤ì¹˜ë§ˆí¬ ìŠ¤í… ìˆ˜ ì¡°ì •
python benchmark.py --warmup-steps 20 --benchmark-steps 100

# torch.compile ë¹„í™œì„±í™” (ë””ë²„ê¹…ìš©)
python benchmark.py --no-compile

# torch.compile ëª¨ë“œ ì„ íƒ
python benchmark.py --compile-mode max-autotune

# Verbose ì¶œë ¥
python benchmark.py --verbose

# ì»¤ìŠ¤í…€ ì¶œë ¥ ê²½ë¡œ
python benchmark.py -o results/my_benchmark.json
```

### í”„ë¡œë•ì…˜ ë²¤ì¹˜ë§ˆí¬

```bash
# ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•œ ê¶Œì¥ ì„¤ì •
python benchmark.py \
  --batch-size 2 \
  --seq-length 512 \
  --warmup-steps 20 \
  --benchmark-steps 100 \
  --compile-mode max-autotune \
  --verbose
```

## ì„¤ì • ì˜µì…˜

### ëª¨ë¸ ì„¤ì •
ê¸°ë³¸ ì„¤ì •ì€ `BenchmarkConfig` í´ë˜ìŠ¤ì— ì •ì˜:

```python
vocab_size: 1000
hidden_size: 512
intermediate_size: 1376
num_hidden_layers: 4
num_attention_heads: 8
num_key_value_heads: 2  # Grouped-Query Attention
max_position_embeddings: 2048
```

### ë²¤ì¹˜ë§ˆí¬ ì„¤ì •

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `batch_size` | 2 | í›ˆë ¨/ì¶”ë¡  ë°°ì¹˜ í¬ê¸° |
| `seq_length` | 512 | ì‹œí€€ìŠ¤ ê¸¸ì´ |
| `num_warmup_steps` | 10 | Warmup ìŠ¤í… ìˆ˜ |
| `num_benchmark_steps` | 50 | ë²¤ì¹˜ë§ˆí¬ ìŠ¤í… ìˆ˜ |
| `gen_batch_size` | 1 | ìƒì„± ë°°ì¹˜ í¬ê¸° |
| `gen_input_length` | 128 | ìƒì„± ì…ë ¥ ê¸¸ì´ |
| `gen_output_length` | 128 | ìƒì„± ì¶œë ¥ ê¸¸ì´ |
| `gen_num_iterations` | 20 | ìƒì„± ë°˜ë³µ íšŸìˆ˜ |

### torch.compile ì„¤ì •

| ëª¨ë“œ | ì„¤ëª… | ì¶”ì²œ ìš©ë„ |
|------|------|----------|
| `default` | ê¸°ë³¸ ìµœì í™” | ì¼ë°˜ì ì¸ ì‚¬ìš© |
| `reduce-overhead` | Overhead ìµœì†Œí™” | ì‘ì€ ë°°ì¹˜ |
| `max-autotune` | ìµœëŒ€ ìµœì í™” | í”„ë¡œë•ì…˜ ë²¤ì¹˜ë§ˆí¬ |

## JSON ê²°ê³¼ êµ¬ì¡°

```json
{
  "timestamp": "2025-10-15T...",
  "config": {
    "batch_size": 2,
    "seq_length": 512,
    "num_warmup_steps": 10,
    "num_benchmark_steps": 50,
    "use_compile": true,
    "compile_mode": "default"
  },
  "default_model": {
    "training": {
      "wall_clock_time": {
        "mean_ms": 45.2,
        "std_ms": 2.1,
        "median_ms": 44.8,
        "min_ms": 42.3,
        "max_ms": 49.1,
        "p95_ms": 47.5,
        "p99_ms": 48.9
      },
      "memory": {
        "peak_mb": 1234.5,
        "final_mb": 987.3,
        "reserved_mb": 1500.0
      },
      "throughput": {
        "samples_per_sec": 44.2,
        "tokens_per_sec": 22691.0
      }
    },
    "inference": { ... },
    "generation": {
      "first_token_latency": {
        "mean_ms": 8.5,
        "p95_ms": 9.2,
        "p99_ms": 9.8
      },
      "per_token_latency": {
        "mean_ms": 2.3,
        "p95_ms": 2.5,
        "p99_ms": 2.7
      },
      "total_generation": {
        "mean_ms": 302.4,
        "output_tokens": 128
      },
      "throughput": {
        "tokens_per_sec": 434.8
      }
    }
  },
  "example_model": { ... },
  "comparison": {
    "training": {
      "speedup": 1.45,
      "time_reduction_percent": 31.2,
      "memory_reduction_mb": 245.3,
      "memory_reduction_percent": 18.5
    },
    "inference": {
      "speedup": 1.62,
      "time_reduction_percent": 38.3,
      "memory_reduction_mb": 189.7
    },
    "generation": {
      "speedup": 1.55,
      "latency_reduction_percent": 35.5,
      "first_token_speedup": 1.48
    }
  }
}
```

## ê²°ê³¼ ë¶„ì„

### jqë¥¼ ì´ìš©í•œ ë¶„ì„

```bash
# ì „ì²´ ìš”ì•½
cat benchmark_results.json | jq '.comparison'

# Training speedup
cat benchmark_results.json | jq '.comparison.training.speedup'

# ë©”ëª¨ë¦¬ ë¹„êµ
cat benchmark_results.json | jq '{
  default_peak: .default_model.training.memory.peak_mb,
  example_peak: .example_model.training.memory.peak_mb,
  reduction_mb: .comparison.training.memory_reduction_mb
}'

# Generation latency
cat benchmark_results.json | jq '{
  default_latency: .default_model.generation.per_token_latency.mean_ms,
  example_latency: .example_model.generation.per_token_latency.mean_ms,
  speedup: .comparison.generation.speedup
}'

# Throughput ë¹„êµ
cat benchmark_results.json | jq '{
  default_tps: .default_model.inference.throughput.tokens_per_sec,
  example_tps: .example_model.inference.throughput.tokens_per_sec
}'
```

### Python ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

```python
import json

with open('benchmark_results.json') as f:
    results = json.load(f)

comp = results['comparison']

print(f"Training Speedup: {comp['training']['speedup']:.2f}x")
print(f"Memory Reduction: {comp['training']['memory_reduction_mb']:.1f} MB")
print(f"Generation Speedup: {comp['generation']['speedup']:.2f}x")
```

## ì¼ë°˜ì ì¸ ìµœì í™” ê²°ê³¼

### Flash Attention 2 ì ìš©

**ì˜ˆìƒ ê°œì„ :**
- Training: 1.3-1.5x speedup
- Inference: 1.5-2.0x speedup
- Memory: 20-30% reduction

### Kernel Fusion (Liger Kernel)

**ì˜ˆìƒ ê°œì„ :**
- Training: 1.2-1.4x speedup
- Memory: 15-25% reduction

### torch.compile

**ì˜ˆìƒ ê°œì„ :**
- Training: 1.1-1.3x speedup
- Inference: 1.2-1.5x speedup

### ì¢…í•© ìµœì í™”

**ëª©í‘œ:**
- Training: 1.5-2.0x speedup
- Inference: 2.0-3.0x speedup
- Memory: 30-40% reduction
- Generation: 2.0-2.5x speedup

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### CUDA OOM

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python benchmark.py --batch-size 1 --seq-length 256

# Compile ë¹„í™œì„±í™”
python benchmark.py --no-compile
```

### Compile ì˜¤ë¥˜

```bash
# Compile ëª¨ë“œ ë³€ê²½
python benchmark.py --compile-mode default

# Compile ë¹„í™œì„±í™”
python benchmark.py --no-compile
```

### ëŠë¦° ì²« ì‹¤í–‰

- ì •ìƒì…ë‹ˆë‹¤! torch.compileê³¼ Triton kernelì˜ ìµœì í™” ê³¼ì •
- Warmup í›„ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë¨

### ë¶ˆì•ˆì •í•œ ì¸¡ì •

```bash
# Warmupê³¼ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í… ìˆ˜ ì¦ê°€
python benchmark.py --warmup-steps 20 --benchmark-steps 100
```

## CI/CD í†µí•©

### GitHub Actions ì˜ˆì‹œ

```yaml
name: Performance Benchmark
on: [push]

jobs:
  benchmark:
    runs-on: [self-hosted, gpu]
    steps:
      - uses: actions/checkout@v2
      - name: Run benchmark
        run: |
          python benchmark.py --output results/benchmark_${{ github.sha }}.json
      - name: Upload results
        uses: actions/upload-artifact@v2
        with:
          name: benchmark-results
          path: results/
```

## ì°¸ê³  ìë£Œ

- torch.compile: https://pytorch.org/docs/stable/torch.compiler.html
- Liger Kernel: https://github.com/linkedin/Liger-Kernel
- Flash Attention 2: https://github.com/Dao-AILab/flash-attention
- CUDA Best Practices: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
