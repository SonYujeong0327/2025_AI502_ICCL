# Performance Benchmark

## ë¹ ë¥¸ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í–‰ (JSON ê²°ê³¼ ì €ì¥, ìš”ì•½ ì¶œë ¥)
python benchmark.py

# ìƒì„¸ ì¶œë ¥
python benchmark.py --verbose

# ì»¤ìŠ¤í…€ ì„¤ì •
python benchmark.py --batch-size 4 --seq-length 1024 --benchmark-steps 100
```

## ì¸¡ì • í•­ëª©

### 1. Wall Clock Time â±ï¸
- **Training**: Forward + Backward pass ì‹œê°„
- **Inference**: Forward passë§Œ
- í†µê³„: mean, median, std, p95, p99

### 2. Memory Consumption ğŸ’¾
- **Peak Memory**: ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **Current Memory**: í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬
- **Reserved Memory**: GPUì— ì˜ˆì•½ëœ ì „ì²´ ë©”ëª¨ë¦¬

### 3. Generation Latency ğŸš€
- **First Token Latency**: Prefill ì‹œê°„ (ì „ì²´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬)
- **Per-token Latency**: Decode ì‹œê°„ (í† í° í•˜ë‚˜ ìƒì„±)
- **Throughput**: tokens/sec

## ê²°ê³¼ ì˜ˆì‹œ

```
ğŸ“Š Training Performance
  Speedup: 1.45x
  Time Reduction: 31.2%
  Memory Reduction: 245.3 MB (18.5%)

ğŸ“Š Inference Performance
  Speedup: 1.62x
  Time Reduction: 38.3%
  Memory Reduction: 189.7 MB

ğŸ“Š Generation Latency
  Per-token Speedup: 1.55x
  Latency Reduction: 35.5%
  First Token Speedup: 1.48x
```

## ì˜µì…˜

| ì˜µì…˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `--verbose`, `-v` | False | ìƒì„¸ ì¶œë ¥ |
| `--output`, `-o` | `benchmark_results.json` | ê²°ê³¼ ì €ì¥ ê²½ë¡œ |
| `--batch-size` | 2 | ë°°ì¹˜ í¬ê¸° |
| `--seq-length` | 512 | ì‹œí€€ìŠ¤ ê¸¸ì´ |
| `--warmup-steps` | 10 | Warmup ìŠ¤í… ìˆ˜ |
| `--benchmark-steps` | 50 | ë²¤ì¹˜ë§ˆí¬ ìŠ¤í… ìˆ˜ |
| `--no-compile` | False | torch.compile ë¹„í™œì„±í™” |
| `--compile-mode` | `default` | compile ëª¨ë“œ |

## Warmupì˜ ì¤‘ìš”ì„±

**10 step warmup**ì„ ìˆ˜í–‰í•˜ëŠ” ì´ìœ :

1. **torch.compile ìµœì í™”**
   - ì²« ì‹¤í–‰ ì‹œ CUDA graph ìƒì„±
   - Kernel fusion ìµœì í™”
   - ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ ìµœì í™”

2. **Triton Kernel ìµœì í™”**
   - GPU ì•„í‚¤í…ì²˜ë³„ íŠœë‹
   - Autotuning ì™„ë£Œ
   - Kernel cache ìƒì„±

3. **CUDA ì´ˆê¸°í™”**
   - cuBLAS/cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
   - GPU memory pool ì„¤ì •

## JSON ê²°ê³¼ êµ¬ì¡°

```json
{
  "timestamp": "2025-10-15T...",
  "config": {
    "batch_size": 2,
    "seq_length": 512,
    "num_warmup_steps": 10,
    "use_compile": true
  },
  "default_model": {
    "training": {
      "wall_clock_time": { "mean_ms": 45.2, ... },
      "memory": { "peak_mb": 1234.5, ... },
      "throughput": { "tokens_per_sec": 22691 }
    },
    "inference": { ... },
    "generation": {
      "first_token_latency": { ... },
      "per_token_latency": { ... }
    }
  },
  "example_model": { ... },
  "comparison": {
    "training": { "speedup": 1.45, ... },
    "inference": { "speedup": 1.62, ... },
    "generation": { "speedup": 1.55, ... }
  }
}
```

## ë¶„ì„ íŒ

```bash
# ìš”ì•½ë§Œ ë³´ê¸°
cat benchmark_results.json | jq '.comparison'

# Training speedup í™•ì¸
cat benchmark_results.json | jq '.comparison.training.speedup'

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
cat benchmark_results.json | jq '{
  default: .default_model.training.memory.peak_mb,
  example: .example_model.training.memory.peak_mb
}'

# Generation latency ë¹„êµ
cat benchmark_results.json | jq '{
  default: .default_model.generation.per_token_latency.mean_ms,
  example: .example_model.generation.per_token_latency.mean_ms
}'
```

## ì°¸ê³ ì‚¬í•­

- ëœë¤ ë°ì´í„° ì‚¬ìš© (ì‹¤ì œ ìƒì„± ë¶ˆí•„ìš”)
- Mixed precision (bfloat16) ì‚¬ìš©
- CUDA synchronizationìœ¼ë¡œ ì •í™•í•œ ì‹œê°„ ì¸¡ì •
- ê° ë²¤ì¹˜ë§ˆí¬ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰
