{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9f58b91",
      "metadata": {},
      "source": [
        "# Flash Attention 2 & Fused Kernel Benchmark\n",
        "\n",
        "이 노트북은 동일한 가중치에서 eager(기본)와 fused(최적화) 커널을 일관되게 비교하고, 최종적으로 실 서비스에 가까운 사전 학습 언어 모델에서 속도와 정확도를 확인합니다. 모든 실험은 GPU에서 실행되며, 양쪽 모델 모두 `torch.compile`을 적용해 공정하게 측정합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5feaac6",
      "metadata": {},
      "source": [
        "## 실험 개요\n",
        "- Flash Attention 2 vs. Eager Multi-Head Attention\n",
        "- RMSNorm / SwiGLU MLP / Linear+CrossEntropy (eager vs. fused)\n",
        "- 전체 Transformer (무작위 가중치) 비교\n",
        "- `meta-llama/Llama-3.2-1B-Instruct` 체크포인트를 활용한 엔드-투-엔드 추론 성능 비교\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd73346e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "from functools import partial\n",
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError('이 노트북은 GPU(CUDA) 환경에서만 실행할 수 있습니다.')\n",
        "\n",
        "device = torch.device('cuda')\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "print(f'Device: {device}')\n",
        "print(f'Dtype: {dtype}')\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'Transformers: {transformers.__version__}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047f7735",
      "metadata": {},
      "source": [
        "## 설정 및 모듈 로드\n",
        "기본(eager) 커널과 fused 커널을 동시에 가져와 비교합니다. 둘 다 동일한 `ExampleConfig`를 사용해 가중치를 공유합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f2a4a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.default_config import TransformerConfig\n",
        "from models.example_config import ExampleConfig\n",
        "from models.default_layers import MultiHeadAttention, RMSNorm as DefaultRMSNorm, SwiGLUMLP as DefaultSwiGLUMLP, fused_linear_cross_entropy as default_linear_ce, RotaryEmbedding\n",
        "from models.example_layers import FlashMultiHeadAttention, RMSNorm as FusedRMSNorm, SwiGLUMLP as FusedSwiGLUMLP, fused_linear_cross_entropy as fused_linear_ce\n",
        "from models.default_model import TransformerForCausalLM\n",
        "from models.example_model import ExampleTransformerForCausalLM\n",
        "\n",
        "SEQ_LEN = 1024\n",
        "BATCH_SIZE = 4\n",
        "PAD_TO = SEQ_LEN - 128\n",
        "COMPILE_MODE = 'reduce-overhead'\n",
        "\n",
        "config = ExampleConfig(\n",
        "    hidden_size=1024,\n",
        "    intermediate_size=4096,\n",
        "    num_hidden_layers=1,\n",
        "    num_attention_heads=16,\n",
        "    num_key_value_heads=16,\n",
        "    max_position_embeddings=max(SEQ_LEN, 2048),\n",
        "    attention_dropout=0.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab7efde",
      "metadata": {},
      "source": [
        "## 공용 유틸리티\n",
        "- `compile_module`: `torch.compile`을 적용(없을 경우 경고)\n",
        "- `benchmark_callable`: GPU 싱크를 포함한 일관된 벤치마크\n",
        "- 가중치 복제 및 텐서 샘플링 헬퍼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9558ba88",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compile_module(module, mode=COMPILE_MODE):\n",
        "    if hasattr(torch, 'compile'):\n",
        "        try:\n",
        "            return torch.compile(module, mode=mode)\n",
        "        except RuntimeError as err:\n",
        "            print(f'torch.compile 실패: {err}. 원본 모듈을 반환합니다.')\n",
        "            return module\n",
        "    else:\n",
        "        print('torch.compile이 지원되지 않아 원본 모듈을 사용합니다.')\n",
        "        return module\n",
        "\n",
        "def benchmark_callable(callable_obj, *call_args, warmup=10, iters=50, **call_kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(warmup):\n",
        "            callable_obj(*call_args, **call_kwargs)\n",
        "    torch.cuda.synchronize()\n",
        "    start = perf_counter()\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(iters):\n",
        "            callable_obj(*call_args, **call_kwargs)\n",
        "    torch.cuda.synchronize()\n",
        "    end = perf_counter()\n",
        "    return (end - start) / iters\n",
        "\n",
        "def sample_attention_inputs(config, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, pad_to=PAD_TO, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size, device=device, dtype=dtype)\n",
        "    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "    rotary = RotaryEmbedding(config).to(device=device)\n",
        "    cos, sin = rotary(hidden_states, position_ids)\n",
        "    cos = cos.to(dtype=dtype)\n",
        "    sin = sin.to(dtype=dtype)\n",
        "    attention_mask = torch.ones(batch_size, seq_len, device=device, dtype=torch.bool)\n",
        "    if pad_to is not None and pad_to < seq_len:\n",
        "        attention_mask[:, pad_to:] = 0\n",
        "    return hidden_states, (cos, sin), attention_mask\n",
        "\n",
        "def diff_stats(a, b):\n",
        "    diff = (a - b).abs()\n",
        "    return diff.max().item(), diff.mean().item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b85600",
      "metadata": {},
      "source": [
        "## Flash Attention 2 vs Eager Multi-Head Attention\n",
        "동일한 가중치를 공유하는 두 모듈을 `torch.compile`로 컴파일한 뒤, padding이 포함된 배치에서 속도와 수치 차이를 비교합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499f6f60",
      "metadata": {},
      "outputs": [],
      "source": [
        "hidden_states, position_embeddings, attention_mask = sample_attention_inputs(config)\n",
        "\n",
        "baseline_attn = MultiHeadAttention(config, layer_idx=0, is_causal=True).to(device=device, dtype=dtype)\n",
        "flash_attn = FlashMultiHeadAttention(config, layer_idx=0, is_causal=True).to(device=device, dtype=dtype)\n",
        "flash_attn.load_state_dict(baseline_attn.state_dict(), strict=False)\n",
        "baseline_attn.eval()\n",
        "flash_attn.eval()\n",
        "\n",
        "baseline_attn = compile_module(baseline_attn)\n",
        "flash_attn = compile_module(flash_attn)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    baseline_out = baseline_attn(hidden_states, position_embeddings, attention_mask=attention_mask)\n",
        "    flash_out = flash_attn(hidden_states, position_embeddings, attention_mask=attention_mask)\n",
        "\n",
        "max_diff, mean_diff = diff_stats(baseline_out, flash_out)\n",
        "baseline_time = benchmark_callable(baseline_attn, hidden_states, position_embeddings, attention_mask=attention_mask)\n",
        "flash_time = benchmark_callable(flash_attn, hidden_states, position_embeddings, attention_mask=attention_mask)\n",
        "speedup = baseline_time / max(flash_time, 1e-12)\n",
        "\n",
        "print(f'Eager attention:  {baseline_time * 1e3:.2f} ms')\n",
        "print(f'Flash attention: {flash_time * 1e3:.2f} ms')\n",
        "print(f'Speedup:         {speedup:.2f}x')\n",
        "print(f'Max abs diff:    {max_diff:.3e}')\n",
        "print(f'Mean abs diff:   {mean_diff:.3e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe380096",
      "metadata": {},
      "source": [
        "## RMSNorm / SwiGLU MLP / Linear+CE (Eager vs Fused)\n",
        "각 커널을 동일한 가중치로 초기화하고 `torch.compile`을 적용한 뒤, 속도와 오차를 비교합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e1bcf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "rms_default = DefaultRMSNorm(config.hidden_size, eps=config.rms_norm_eps).to(device=device, dtype=dtype)\n",
        "rms_fused = FusedRMSNorm(config.hidden_size, eps=config.rms_norm_eps).to(device=device, dtype=dtype)\n",
        "rms_fused.load_state_dict(rms_default.state_dict(), strict=False)\n",
        "\n",
        "mlp_default = DefaultSwiGLUMLP(config).to(device=device, dtype=dtype)\n",
        "mlp_fused = FusedSwiGLUMLP(config).to(device=device, dtype=dtype)\n",
        "mlp_fused.load_state_dict(mlp_default.state_dict(), strict=False)\n",
        "\n",
        "rms_default = compile_module(rms_default)\n",
        "rms_fused = compile_module(rms_fused)\n",
        "mlp_default = compile_module(mlp_default)\n",
        "mlp_fused = compile_module(mlp_fused)\n",
        "\n",
        "norm_inputs = torch.randn(BATCH_SIZE, SEQ_LEN, config.hidden_size, device=device, dtype=dtype)\n",
        "mlp_inputs = norm_inputs.clone()\n",
        "ce_hidden = torch.randn(BATCH_SIZE * SEQ_LEN, config.hidden_size, device=device, dtype=dtype)\n",
        "ce_labels = torch.randint(0, config.vocab_size, (BATCH_SIZE * SEQ_LEN,), device=device)\n",
        "lm_head_weight = torch.randn(config.vocab_size, config.hidden_size, device=device, dtype=dtype)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    rms_eager = rms_default(norm_inputs)\n",
        "    rms_fused_out = rms_fused(norm_inputs)\n",
        "    mlp_eager = mlp_default(mlp_inputs)\n",
        "    mlp_fused_out = mlp_fused(mlp_inputs)\n",
        "    ce_eager = default_linear_ce(ce_hidden, ce_labels, lm_head_weight)\n",
        "    ce_fused = fused_linear_ce(ce_hidden, ce_labels, lm_head_weight)\n",
        "\n",
        "rms_diff = diff_stats(rms_eager, rms_fused_out)\n",
        "mlp_diff = diff_stats(mlp_eager, mlp_fused_out)\n",
        "ce_gap = abs(ce_fused.item() - ce_eager.item())\n",
        "\n",
        "rms_time_eager = benchmark_callable(rms_default, norm_inputs)\n",
        "rms_time_fused = benchmark_callable(rms_fused, norm_inputs)\n",
        "mlp_time_eager = benchmark_callable(mlp_default, mlp_inputs)\n",
        "mlp_time_fused = benchmark_callable(mlp_fused, mlp_inputs)\n",
        "ce_time_eager = benchmark_callable(default_linear_ce, ce_hidden, ce_labels, lm_head_weight)\n",
        "ce_time_fused = benchmark_callable(fused_linear_ce, ce_hidden, ce_labels, lm_head_weight)\n",
        "\n",
        "print('=== RMSNorm (Eager vs Fused) ===')\n",
        "print(f'Eager: {rms_time_eager * 1e3:.2f} ms | Fused: {rms_time_fused * 1e3:.2f} ms | Speedup: {rms_time_eager / max(rms_time_fused, 1e-12):.2f}x')\n",
        "print(f'Max diff: {rms_diff[0]:.3e} | Mean diff: {rms_diff[1]:.3e}')\n",
        "\n",
        "print('\n",
        "=== SwiGLU MLP (Eager vs Fused) ===')\n",
        "print(f'Eager: {mlp_time_eager * 1e3:.2f} ms | Fused: {mlp_time_fused * 1e3:.2f} ms | Speedup: {mlp_time_eager / max(mlp_time_fused, 1e-12):.2f}x')\n",
        "print(f'Max diff: {mlp_diff[0]:.3e} | Mean diff: {mlp_diff[1]:.3e}')\n",
        "\n",
        "print('\n",
        "=== Linear + CrossEntropy ===')\n",
        "print(f'Eager: {ce_time_eager * 1e3:.2f} ms | Fused: {ce_time_fused * 1e3:.2f} ms | Speedup: {ce_time_eager / max(ce_time_fused, 1e-12):.2f}x')\n",
        "print(f'Loss gap: {ce_gap:.3e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a270a2f8",
      "metadata": {},
      "source": [
        "## 전체 Transformer (Synthetic) 비교\n",
        "무작위 초기화된 동일한 가중치에서 전체 모델 forward와 loss 계산을 비교합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ccca8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "default_model = TransformerForCausalLM(config).to(device=device, dtype=dtype)\n",
        "optimized_model = ExampleTransformerForCausalLM(config).to(device=device, dtype=dtype)\n",
        "optimized_model.load_state_dict(default_model.state_dict(), strict=False)\n",
        "\n",
        "default_model.eval()\n",
        "optimized_model.eval()\n",
        "default_model = compile_module(default_model)\n",
        "optimized_model = compile_module(optimized_model)\n",
        "\n",
        "input_ids = torch.randint(0, config.vocab_size, (BATCH_SIZE, SEQ_LEN), device=device)\n",
        "labels = input_ids.clone()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    eager_out = default_model(input_ids=input_ids, labels=labels)\n",
        "    fused_out = optimized_model(input_ids=input_ids, labels=labels)\n",
        "\n",
        "logits_diff = diff_stats(eager_out.logits, fused_out.logits)\n",
        "loss_gap = abs((fused_out.loss - eager_out.loss).item())\n",
        "\n",
        "forward_time_eager = benchmark_callable(lambda: default_model(input_ids=input_ids, labels=labels))\n",
        "forward_time_fused = benchmark_callable(lambda: optimized_model(input_ids=input_ids, labels=labels))\n",
        "speedup = forward_time_eager / max(forward_time_fused, 1e-12)\n",
        "\n",
        "print(f'Eager forward:   {forward_time_eager * 1e3:.2f} ms')\n",
        "print(f'Fused forward:  {forward_time_fused * 1e3:.2f} ms')\n",
        "print(f'Speedup:         {speedup:.2f}x')\n",
        "print(f'Max logit diff:  {logits_diff[0]:.3e}')\n",
        "print(f'Mean logit diff: {logits_diff[1]:.3e}')\n",
        "print(f'Loss gap:        {loss_gap:.3e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d481551",
      "metadata": {},
      "source": [
        "## 실제 체크포인트 로드: meta-llama/Llama-3.2-1B-Instruct\n",
        "다음 셀은 Hugging Face 허브에서 사전 학습 모델과 토크나이저를 로드한 뒤, eager Transformer와 fused Transformer에 동일한 가중치를 로드하고 성능을 측정합니다. (인터넷 접근이 필요합니다.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5142b128",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct', use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "prompts = [\n",
        "    'Explain the Flash Attention algorithm to a senior ML engineer in three bullet points.',\n",
        "    'Summarize the benefits of fused MLP kernels in transformer inference.',\n",
        "]\n",
        "batch = tokenizer(prompts, return_tensors='pt', padding=True).to(device)\n",
        "prefill_length = batch['input_ids'].shape[-1]\n",
        "print(f'Prompt length: {prefill_length}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4811c523",
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pretrained = TransformerForCausalLM.from_pretrained(\n",
        "    'meta-llama/Llama-3.2-1B-Instruct',\n",
        "    torch_dtype=dtype,\n",
        "    device_map={'': str(device)},\n",
        ")\n",
        "example_pretrained = ExampleTransformerForCausalLM.from_pretrained(\n",
        "    'meta-llama/Llama-3.2-1B-Instruct',\n",
        "    torch_dtype=dtype,\n",
        "    device_map={'': str(device)},\n",
        ")\n",
        "\n",
        "default_pretrained.eval()\n",
        "example_pretrained.eval()\n",
        "\n",
        "default_pretrained = compile_module(default_pretrained)\n",
        "example_pretrained = compile_module(example_pretrained)\n",
        "\n",
        "def benchmark_generation(model, input_ids, attention_mask, max_new_tokens=64, warmup=2, iters=5):\n",
        "    torch.cuda.synchronize()\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(warmup):\n",
        "            model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                use_cache=True,\n",
        "            )\n",
        "    torch.cuda.synchronize()\n",
        "    start = perf_counter()\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(iters):\n",
        "            model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                use_cache=True,\n",
        "            )\n",
        "    torch.cuda.synchronize()\n",
        "    end = perf_counter()\n",
        "    return (end - start) / iters\n",
        "\n",
        "default_time = benchmark_generation(default_pretrained, batch['input_ids'], batch['attention_mask'])\n",
        "example_time = benchmark_generation(example_pretrained, batch['input_ids'], batch['attention_mask'])\n",
        "tokens_generated = batch['input_ids'].shape[0] * 64\n",
        "throughput_default = tokens_generated / max(default_time, 1e-12)\n",
        "throughput_example = tokens_generated / max(example_time, 1e-12)\n",
        "\n",
        "print(f'Eager model:  {default_time:.3f} s per batch | Throughput: {throughput_default:.1f} tokens/s')\n",
        "print(f'Fused model: {example_time:.3f} s per batch | Throughput: {throughput_example:.1f} tokens/s')\n",
        "print(f'Speedup:      {default_time / max(example_time, 1e-12):.2f}x')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ecafa1d",
      "metadata": {},
      "source": [
        "## 요약 및 다음 단계\n",
        "- 모든 커널을 `torch.compile`로 감싼 상태에서 GPU에서 일관되게 비교했습니다.\n",
        "- Flash Attention 2 및 fused 커널이 eager 대비 어떤 속도 향상을 주는지 즉시 확인할 수 있습니다.\n",
        "- Hugging Face 체크포인트를 이용해 실제 언어 모델 추론에서도 성능 차이를 측정할 수 있습니다.\n",
        "\n",
        "### 권장 후속 작업\n",
        "1. 실제 워크로드(프롬프트 길이, 배치 크기)를 반영해 파라미터를 수정\n",
        "2. `torch.compile` 모드를 `max-autotune` 등으로 변경해 최적 지점을 탐색\n",
        "3. 추가 fused 커널(예: attention mask fusing, layernorm)을 연결해 전체 파이프라인을 튜닝\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}